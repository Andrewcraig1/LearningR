table(test$Vandal, pred.prob >= 0.5)
(618+12)/nrow(test)
source('~/U5 Assign 1 - wiki.R')
str(wiki)
View(wikiWords)
View(wiki)
wikiWords2 <- wikiWords
wikiWords2$HTTP <- ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
table(wikiWords2$HTTP)
wikiTrain2 <- subset(wikiWords2, spl==TRUE)
wikiTest2 <- subset(wikiWords2, spl==FALSE)
wikiCART2 <- rpart(Vandal~., data=wikiTrain2, method="class")
prp(wikiCART2)
pred2 <- predict(wikiCART2, newdata=wikiTest2)
pred.prob2 <- pred2[,2]
table(wikiTest2$Vandal, pred.prob2 >= 0.5)
(609+57)/nrow(wikiTest2)
wikiWords2$NumWordsAdded <- rowSums(as.matrix(dtm))
wikiWords2$NumWordsRemoved <- rowSums(as.matrix(dtm2))
summary(wikiWords2$NumWordsAdded)
wikiTrain2 <- subset(wikiWords2, spl==TRUE)
wikiTest2 <- subset(wikiWords2, spl==FALSE)
wikiCART2 <- rpart(Vandal~., data=wikiTrain2, method="class")
pred2 <- predict(wikiCART2, newdata=wikiTest2)
pred.prob2 <- pred2[,2]
table(wikiTest2$Vandal, pred.prob2 >= 0.5)
(514+248)/nrow(wikiTest2)
source('~/U5 Assign 1 - wiki.R')
wikiWords3 <- wikiWords2
wikiWords3$Minor <- wiki$Minor
wikiWords3$Loggedin <- wiki$Loggedin
wikiTrain3 <- subset(wikiWords3, spl==TRUE)
wikiTest3 <- subset(wikiWords3, spl==FALSE)
wikiCART3 <- rpart(Vandal~., data=wikiTrain3, method="class")
prp(wikiCART3)
pred3 <- predict(wikiCART3, newdata=wikiTest3)
pred.prob3 <- pred3[,2]
table(wikiTest3$Vandal, pred.prob3 >= 0.5)
(595+241)/nrow(wikiTest3)
source('~/U5 Assign 1 - wiki.R')
source('~/U5 Assign 1 - wiki.R')
setwd("~/Documents/MOOCs/Analytics Edge/Unit 5")
Sys.setlocale("LC_ALL", "C")
clinical <- read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
str(emails)
str(clinical)
max(clinical$abstract)
which.max(clinical$abstract)
View(clinical)
View(clinical)
summary(clinical$abstract)
trials <- read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
rm(clinical)
trials <- read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
str(trials)
summary(trials$abstract)
summary(trials)
nchar(trials$abstract)
max(nchar(trials$abstract))
summary(nchar(trials$abstract))
table(nchar(trials$abstract))
# max(trials$title)
max(trials$title)
min(trials$title)
min(nchar(trials$title))
which.min(trials$title)
which.min(nchar(trials$title))
trials$title[1258]
library(tm)
names(trials)
corpusTitle <- Corpus(VectorSource(trials$title))
corpusAbstract <- Corpus(VectorSource(trials$abstract))
corpusTitle <- tm_map(corpusTitle, tolower)
corpusTitle <- tm_map(corpusTitle, PlainTextDocument)
corpusAbstract <- tm_map(corpusAbstract, tolower)
corpusAbstract <- tm_map(corpusAbstract, PlainTextDocument)
corpusAbstract <- tm_map(corpusAbstract, tolower)
corpusAbstract <- tm_map(corpusAbstract, PlainTextDocument)
corpusTitle <- tm_map(corpusTitle, removePunctuation)
corpusAbstract <- tm_map(corpusAbstract, removePunctuation)
corpusTitle <- tm_map(corpusTitle, removeWords, stopwords("english"))
corpusAbstract <- tm_map(corpusAbstract, removeWords, stopwords("english"))
corpusTitle <- tm_map(corpusTitle, stemDocument)
corpusAbstract <- tm_map(corpusAbstract, stemDocument)
dtmTitle <- DocumentTermMatrix(corpusTitle)
dtmAbstract <- DocumentTermMatrix(corpusAbstract)
dtmTitle
dtmAbstract
dtmTitle
dtmTitle <- removeSparseTerms(dtmTitle, 0.95)
dtmAbstract <- removeSparseTerms(dtmAbstract, 0.95)
dtmTitle
dtmAbstract
length(stopwords("english"))
names(trials)
# Create corpus
names(trials)
corpusTitle <- Corpus(VectorSource(trials$title))
corpusAbstract <- Corpus(VectorSource(trials$abstract))
# Pre-process data
corpusTitle <- tm_map(corpusTitle, tolower)
corpusTitle <- tm_map(corpusTitle, PlainTextDocument)
corpusAbstract <- tm_map(corpusAbstract, tolower)
corpusAbstract <- tm_map(corpusAbstract, PlainTextDocument)
corpusTitle <- tm_map(corpusTitle, removePunctuation)
corpusAbstract <- tm_map(corpusAbstract, removePunctuation)
corpusTitle <- tm_map(corpusTitle, removeWords, stopwords("english"))
corpusAbstract <- tm_map(corpusAbstract, removeWords, stopwords("english"))
corpusTitle <- tm_map(corpusTitle, stemDocument)
corpusAbstract <- tm_map(corpusAbstract, stemDocument)
#  Now the Added wiki in this corpus are ready for our machine learning algorithms.
# Create matrix: dtm means document-term matricies
dtmTitle <- DocumentTermMatrix(corpusTitle)
dtmAbstract <- DocumentTermMatrix(corpusAbstract)
dtmTitle
dtmAbstract
# Remove sparse terms. In this recitation remove terms that don't appear in at least 5% of the documents
dtmTitle <- removeSparseTerms(dtmTitle, 0.95)
dtmAbstract <- removeSparseTerms(dtmAbstract, 0.95)
dtmTitle
dtmAbstract
dtmTitle <- as.data.frame(as.matrix(dtmTitle))
dtmAbstract <- as.data.frame(as.matrix(dtmAbstract))
colSums(dtmAbstract)
summary(colSums(dtmAbstract))
which.max(colSums(dtmAbstract))
colnames(dtmTitle) <- paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) <- paste0("A", colnames(dtmAbstract))
dtm <- cbind(dtmTitle, dtmAbstract)
str(dtm)
dtm$trial <- trials$trial
str(dtm)
train <- subset(dtm, spl == TRUE)
set.seed(144)
spl <- sample.split(dtm$trial, 0.7)
train <- subset(dtm, spl == TRUE)
test <- subset(dtm, spl == FALSE)
table(train$trial)
730/(730+572)
library(rpart)
library(rpart.plot)
trialCART <- rpart(trial~., data=train, method="class")
prp(trialCART)
pred <- predict(emailCART, data=train)
pred <- predict(trainCART, data=train)
pred <- predict(trialCART, data=train)
pred[1:10,]
pred.prob <- pred[,2]
max(pred.prob)
table(train$trial, pred.prob >= 0.5)
(631+441)/nrow(train)
(441)/(131+441)
631/(631+99)
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 2 Medicine reviews.R')
pred <- predict(trialCART, newdata=test)
predTest <- predict(trialCART, newdata=test)
predTest[1:10,]
pred.prob <- predTest[,2]
table(test$trial, pred.prob >= 0.5)
(261+162)/nrow(test)
library(ROCR)
predROCR <- prediction(pred.prob, test$trial)
perfROCR <- performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize=TRUE)
performance(predROCR, "auc")@y.values
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 2 Medicine reviews.R')
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 2 Medicine reviews.R')
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 2 Medicine reviews.R')
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 2 Medicine reviews.R')
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 2 Medicine reviews.R')
Sys.setlocale("LC_ALL", "C")
setwd("~/Documents/MOOCs/Analytics Edge/Unit 5")
emails <- read.csv("emails.csv", stringsAsFactors=FALSE)
str(emails)
table(emails$spam)
max(nchar(emails$text))
which.min(nchar(emails$text)
which.min(nchar(emails$text))
which.min(nchar(emails$text))
emails$text[1992]
corpus <- Corpus(VectorSource(emails$text))
corpus[[1]]
strwrap(corpus[1])
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)
dtm
length(stopwords("english"))
spddtm <- removeSparseTerms(dtm, 0.95)
dtm
spddtm
spdtm <- removeSparseTerms(dtm, 0.95)
rm(spddtm)
spdtm
emailsSparse <- as.data.frame(as.matrix(spdtm))
colnames(spdtmTitle) <- colnames(spdtmTitle)
colnames(spdtm) <- colnames(spdtm)
corpus <- Corpus(VectorSource(emails$text))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
dtm <- DocumentTermMatrix(corpus)
dtm
spdtm <- removeSparseTerms(dtm, 0.95)
spdtm
emailsSparse <- as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) <- colnames(emailsSparse)
which.max(colSums(emailsSparse))
emailsSparse$spam <- emails$spam
table(emailsSparse$spam)
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 3 spam emails.R')
frequency(colSums(emailsSparse))
sort(colSums(emailsSparse)>=5000)
sort(colSums(emailsSparse)>=5000, decreasing = TRUE)
sort(colSums(emailsSparse)>=5000)
order(colSums(emailsSparse)>=5000)
frequency(order(colSums(emailsSparse)>=5000))
sort(colSums(subset(emailsSparse, spam == 0)))
sort(colSums(subset(emailsSparse, >=1000,spam == 0)))
sort(colSums(subset(emailsSparse, emailsSparse>=1000,spam == 0)))
sort(colSums(subset(emailsSparse,spam == 1)))
emailsSparse$spam <- as.factor(emailsSparse$spam)
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 3 spam emails.R')
library(caTools)
set.seed(123)
spl <- sample.split(emailsSparse$spam, 0.7)
train <- subset(emailsSparse, spl == TRUE)
test <- subset(emailsSparse, spl == FALSE)
emailsSparseLog<-glm(spam ~ ., data=train, family = "binomial")
emailsSparseLog<-glm(spam ~ ., data=train, family = "binomial")
spamLog<-glm(spam ~ ., data=train, family = "binomial")
rm(emailsSparseLog)
library(rpart)
library(rpart.plot)
spamCART <- rpart(spam ~ ., data=train, method="class")
prp(spamCART)
library(randomForest)
set.seed(123)
spamRF <- randomForest(spam ~ ., data=train)
spamRF <- randomForest(spam ~ ., data=train)
spamRF <- randomForest(spam ~ ., data=train)
library(randomForest)
set.seed(123)
spamRF <- randomForest(spam ~ ., data=train)
spamRF <- randomForest(spam ~ ., data=train)
library(randomForest)
set.seed(123)
spamRF<-randomForest(spam~ ., data = train)
library(caTools)
library(randomForest)
set.seed(123)
spamRF <- randomForest(spam ~ ., data=train)
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 3 spam emails.R')
# # Run Language Settings
Sys.setlocale("LC_ALL", "C")
# Load the dataset. Fot txt data sets add argument " stringsAsFactors=FALSE"
emails <- read.csv("emails.csv", stringsAsFactors=FALSE)
str(emails)
table(emails$spam)
#Q1.5
# Max number of characters in abstrac
max(nchar(emails$text))
which.min(nchar(emails$text))
#1992
emails$text[1992]
###P2 Preparing the Corpus
##P2.1
# Load tm package
library(tm)
# Create corpus
corpus <- Corpus(VectorSource(emails$text))
corpus[[1]]
strwrap(corpus[1])
# Pre-process data
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
#Check stop words = 174
length(stopwords("english"))
# Create matrix
dtm <- DocumentTermMatrix(corpus)
dtm
# no of terms=28687
# Remove sparse terms. In this recitation remove terms that don't appear in at least 5% of the documents
spdtm <- removeSparseTerms(dtm, 0.95)
spdtm
# Now no of terms=330
# Create data frame
emailsSparse <- as.data.frame(as.matrix(spdtm))
#Make variable names
colnames(emailsSparse) <- colnames(emailsSparse)
#P2.3 which.max
which.max(colSums(emailsSparse))
##P2.4
# Add in the outcome variable
emailsSparse$spam <- emails$spam
# We can read the most frequent terms in the ham dataset (ie spam=0)
sort(colSums(subset(emailsSparse, spam == 0)))
# We can read the most frequent terms in the ham dataset (ie spam=1). Dont count the word "spam" as this is the dependent variable.
sort(colSums(subset(emailsSparse,spam == 1)))
###P3 Building machine Learning Models
##p3.1
#convert dependent variable "spam" to a factor
emailsSparse$spam <- as.factor(emailsSparse$spam)
# Split the data
library(caTools)
set.seed(123)
spl <- sample.split(emailsSparse$spam, 0.7)
train <- subset(emailsSparse, spl == TRUE)
test <- subset(emailsSparse, spl == FALSE)
##QQ Build a logistic regression model
spamLog<-glm(spam ~ ., data=train, family = "binomial")
# Build a CART model
library(rpart)
library(rpart.plot)
spamCART <- rpart(spam ~ ., data=train, method="class")
prp(spamCART)
# Random forest model
library(randomForest)
set.seed(123)
spamRF <- randomForest(spam ~ ., data=train)
View(train)
colnames(emailsSparse) <- make.names(colnames(emailsSparse))
#P2.3 which.max
which.max(colSums(emailsSparse))
##P2.4
# Add in the outcome variable
emailsSparse$spam <- emails$spam
# We can read the most frequent terms in the ham dataset (ie spam=0)
sort(colSums(subset(emailsSparse, spam == 0)))
# We can read the most frequent terms in the ham dataset (ie spam=1). Dont count the word "spam" as this is the dependent variable.
sort(colSums(subset(emailsSparse,spam == 1)))
###P3 Building machine Learning Models
##p3.1
#convert dependent variable "spam" to a factor
emailsSparse$spam <- as.factor(emailsSparse$spam)
# Split the data
library(caTools)
set.seed(123)
spl <- sample.split(emailsSparse$spam, 0.7)
train <- subset(emailsSparse, spl == TRUE)
test <- subset(emailsSparse, spl == FALSE)
##QQ Build a logistic regression model
spamLog<-glm(spam ~ ., data=train, family = "binomial")
# Build a CART model
library(rpart)
library(rpart.plot)
spamCART <- rpart(spam ~ ., data=train, method="class")
prp(spamCART)
# Random forest model
library(randomForest)
set.seed(123)
spamRF <- randomForest(spam ~ ., data=train)
spamRF <- randomForest(spam ~ ., data=train)
pred.prob.spamLog <- predict(spamLog, data=training)
pred.spamLog <- predict(spamLog, data=training)
pred.prob.spamLog <- pred.spamLog[,2]
pred.prob.spamLog <- pred.spamLog[,2]
pred.prob.spamLog <- pred.spamLog[,2]
table(pred.spamLog)
table(train$spam, pred.prob.spamLog >= 0.5)
table(train$spam, pred.prob.spamLog < 0.00001)
table(train$spam, pred.prob.spamLog > 0.99999)
4010-954-3056
summary(spamLog)
table(train$spam, pred.spamLog)
summary(spamLog)
View(train)
table(test$spam)
table(train$spam)
table(train$spam, pred.spamLog)
pred_spamLog <- predict(spamLog, data=training)
table(pred_spamLog)
pred.spamLog <- predict(spamLog, data=training)
pred.spamLog <- predict(spamLog, data=training, type="response")
table(pred.spamLog)
# # Run Language Settings
Sys.setlocale("LC_ALL", "C")
# Load the dataset. Fot txt data sets add argument " stringsAsFactors=FALSE"
emails <- read.csv("emails.csv", stringsAsFactors=FALSE)
str(emails)
table(emails$spam)
#Q1.5
# Max number of characters in abstrac
max(nchar(emails$text))
which.min(nchar(emails$text))
#1992
emails$text[1992]
###P2 Preparing the Corpus
##P2.1
# Load tm package
library(tm)
# Create corpus
corpus <- Corpus(VectorSource(emails$text))
corpus[[1]]
strwrap(corpus[1])
# Pre-process data
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
#Check stop words = 174
length(stopwords("english"))
# Create matrix
dtm <- DocumentTermMatrix(corpus)
dtm
# no of terms=28687
# Remove sparse terms. In this recitation remove terms that don't appear in at least 5% of the documents
spdtm <- removeSparseTerms(dtm, 0.95)
spdtm
# Now no of terms=330
# Create data frame
emailsSparse <- as.data.frame(as.matrix(spdtm))
#Make variable names
colnames(emailsSparse) <- make.names(colnames(emailsSparse))
#P2.3 which.max
which.max(colSums(emailsSparse))
##P2.4
# Add in the outcome variable
emailsSparse$spam <- emails$spam
# We can read the most frequent terms in the ham dataset (ie spam=0)
sort(colSums(subset(emailsSparse, spam == 0)))
# We can read the most frequent terms in the ham dataset (ie spam=1). Dont count the word "spam" as this is the dependent variable.
sort(colSums(subset(emailsSparse,spam == 1)))
###P3 Building machine Learning Models
##p3.1
#convert dependent variable "spam" to a factor
emailsSparse$spam <- as.factor(emailsSparse$spam)
# Split the data
library(caTools)
set.seed(123)
spl <- sample.split(emailsSparse$spam, 0.7)
train <- subset(emailsSparse, spl == TRUE)
test <- subset(emailsSparse, spl == FALSE)
##QQ Build a logistic regression model
spamLog<-glm(spam ~ ., data=train, family = "binomial")
summary(spamLog)
pred.spamLog <- predict(spamLog, data=training, type="response")
table(pred.spamLog)
# Build a CART model
library(rpart)
library(rpart.plot)
spamCART <- rpart(spam ~ ., data=train, method="class")
prp(spamCART)
# Random forest model
library(randomForest)
set.seed(123)
spamRF <- randomForest(spam ~ ., data=train)
# Make predictions on the training set for each model
pred.spamLog <- predict(spamLog, data=training, type="response")
table(pred.spamLog)
table(train$spam,, pred.spamLog>= 0.5)
pred.spamLog <- predict(spamLog, data=training, type="response")
table(train$spam,, pred.spamLog >= 0.5)
table(train$spam,, pred.spamLog >== 0.5)
table(train$spam, pred.spamLog >= 0.5)
(3052+954)/nrow(test)
(3052+954)/nrow(train)
library(ROCR)
ROCRpred<-prediction(pred.prob.spamLog,test$spam)
ROCRpred<-prediction(pred.prob.spamLog,train$spam)
as.numeric(performance(ROCRpred, "auc")@y.values)
pred.CART <- predict(spamCART, data=train, type="class")
table(train$spam, pred.CART)
pred.prob.CART <- pred.CART[,2]
(2885+894)/nrow(train)
table(train$spam, pred.CART)
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 3 spam emails.R')
library(ROCR)
predROCR.CART <- prediction(pred.CART, train$spam)
predROCR.CART <- prediction(spamCART, train$spam)
pred.probCART <- pred.CART[,2]
pred.CART <- predict(spamCART, data=train)
pred.probCART <- pred.CART[,2]
pred.probCART <- pred.CART[,2]
predROCR.CART <- prediction(pred.probCART, train$spam)
perfROCR.CART <- performance(predROCR.CART, "tpr", "fpr")
plot(perfROCR.CART, colorize=TRUE)
performance(predROCR.CART, "auc")@y.values
predictRF <- predict(spamRF, data=train)
table(train$spam, predictRF)
(3013+914)/nrow(train)
predictRF <- predict(spamRF, data=train, type = "prob")
table(train$spam, predictRF)
predictRF <- predict(spamRF, data=train)
table(train$spam, predictRF)
pred.probRF <- pred.RF[,2]
predictRF <- predict(spamRF, data=train, type = "prob")
table(train$spam, predictRF)
predictRF <- predict(spamRF, data=train)
table(train$spam, predictRF)
predROCR.RF<- prediction(pred.probCART, train$spam, type = "prob")
pred.probRF <- predRF[,2]
pred.probRF <- predictRF[,2]
predROCR.RF<- prediction(predRF, train$spam, type="prob")
pred.spamLog <- predict(spamLog, data=test, type="response")
summary(spamLog)
pred.spamLog <- predict(spamLog, data=test, type="response")
table(pred.spamLog)
pred.spamLog <- predict(spamLog, data=test, type="response")
table(test$spam, pred.spamLog >= 0.5)
pred.spamLog <- predict(spamLog, data=test, type="response")
pred.spamLog <- predict(spamLog, data=test, type="binomial")
source('~/Documents/MOOCs/Analytics Edge/Unit 5/U5 Assign 3 spam emails.R')
